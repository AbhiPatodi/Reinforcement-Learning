{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The goal of this assignment is to use Reinforcement Learning and Open AI to implement basic Q-Learning model.\n",
    "Using Open AI Gym's Frozen Lake Toy text as an Environment and by using hyperparameter tuning, \n",
    "comparing of performance of the models\n",
    "\n",
    "Using different policies like Off Policy, On Policy compared the average rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model - Off Policy Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ùëÑ (ùë†,ùëé )‚Üê ùëÑ (ùë†,ùëé )+ùõº (ùëü + ùõæ maxùëé‚Ä≤ùëÑ (ùë†‚Ä≤,ùëé‚Ä≤) ‚àí ùëÑ(ùë†,ùëé) ) , where ùëé‚Ä≤ are all actions, that were probed in state ùë†‚Ä≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes agent will play during training \n",
    "num_episodes = 5000\n",
    "\n",
    "# Maximum number of steps agent takes per episode\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "#Alpha - Learning Rate\n",
    "learning_rate = 0.7\n",
    "\n",
    "#Gamma - Discounting Rate\n",
    "discount_rate = 0.8\n",
    "\n",
    "#Epsilon - Exploration Rate\n",
    "exploration_rate = 1.0\n",
    "\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# Exponential Decay Rate\n",
    "exploration_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "\n",
    "rewards_all_episodes = []\n",
    "avg_step = []\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    cnt_step = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        cnt_step = cnt_step+1 \n",
    "        \n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "                \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "        \n",
    "        # Add new reward        \n",
    "        if done == True: \n",
    "            break\n",
    "    #Appending the average step\n",
    "    avg_step.append(cnt_step)    \n",
    "        \n",
    "    # Exploration rate decay   \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/500)\n",
    "count = 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Step taken: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Step taken:\",int(sum(avg_step)/len(avg_step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "500 :  0.11000000000000008\n",
      "1000 :  0.3960000000000003\n",
      "1500 :  0.2600000000000002\n",
      "2000 :  0.23600000000000018\n",
      "2500 :  0.19000000000000014\n",
      "3000 :  0.3060000000000002\n",
      "3500 :  0.3040000000000002\n",
      "4000 :  0.3920000000000003\n",
      "4500 :  0.3720000000000003\n",
      "5000 :  0.2740000000000002\n"
     ]
    }
   ],
   "source": [
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/500)))\n",
    "    count += 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[2.30019471e-04 1.61307959e-03 2.31271883e-03 1.66758174e-03]\n",
      " [6.96994883e-04 1.95532216e-04 1.76722562e-03 1.57076801e-03]\n",
      " [4.52504048e-02 9.26105056e-05 1.33484041e-03 1.76946772e-03]\n",
      " [6.81166040e-04 1.48645933e-05 1.24341525e-05 2.05627645e-03]\n",
      " [4.03582378e-03 4.11546464e-05 2.11432848e-04 2.04704168e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.70267181e-04 3.19237036e-07 2.38967723e-02 2.63931839e-09]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.91118147e-03 1.91836552e-06 7.87043709e-04 1.04935599e-02]\n",
      " [3.99938518e-03 2.74621807e-01 4.51166987e-04 2.15368680e-02]\n",
      " [1.49040984e-03 4.25047179e-02 1.21329761e-04 5.62567773e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.23513226e-02 1.33543301e-02 2.28868571e-02 9.33135924e-04]\n",
      " [2.83843889e-02 1.65490783e-02 8.69664644e-01 6.80821500e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the baseline model with maximum number of 5000 episodes we got around 27% accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model - On Policy SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ùëÑ (ùë†,ùëé) ‚Üê ùëÑ (ùë†,ùëé) + ùõº (ùëü + ùõæ ùëÑ (ùë†‚Ä≤,ùëé‚Ä≤ )‚àí ùëÑ (ùë†,ùëé) ) , where ùëé‚Ä≤ is the action, that was taken according to policy ùúã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes agent will play during training \n",
    "num_episodes = 5000\n",
    "\n",
    "# Maximum number of steps agent takes per episode\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "#Alpha - Learning Rate\n",
    "learning_rate = 0.7\n",
    "\n",
    "#Gamma - Discounting Rate\n",
    "discount_rate = 0.8\n",
    "\n",
    "#Epsilon - Exploration Rate\n",
    "exploration_rate = 1.0\n",
    "\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# Exponential Decay Rate\n",
    "exploration_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "\n",
    "rewards_all_episodes = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "                \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table\n",
    "        temp_lst = [] # This list holds the value for every action on new state.\n",
    "        \n",
    "        for i in range(env.action_space.n):\n",
    "            temp_lst.append(q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * (q_table[new_state,i])))\n",
    "        q_table[state, action] = max(temp_lst)\n",
    "        \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "        \n",
    "        # Add new reward        \n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Exploration rate decay   \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/500)\n",
    "count = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "500 :  0.09000000000000007\n",
      "1000 :  0.3140000000000002\n",
      "1500 :  0.33400000000000024\n",
      "2000 :  0.3140000000000002\n",
      "2500 :  0.32200000000000023\n",
      "3000 :  0.2820000000000002\n",
      "3500 :  0.34000000000000025\n",
      "4000 :  0.33800000000000024\n",
      "4500 :  0.34200000000000025\n",
      "5000 :  0.4240000000000003\n"
     ]
    }
   ],
   "source": [
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/500)))\n",
    "    count += 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[1.76355638e-03 2.07642054e-03 7.20056865e-03 1.09527494e-03]\n",
      " [5.83786640e-04 4.75403974e-04 9.47137511e-04 5.64832348e-03]\n",
      " [1.26293343e-03 6.18169003e-03 1.28801045e-03 2.29554626e-04]\n",
      " [1.36329256e-04 9.72826692e-06 2.08270708e-05 3.44877577e-03]\n",
      " [7.02222568e-02 1.96335682e-03 1.50710803e-03 5.78081030e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.39515805e-05 5.68184090e-05 7.65012200e-03 2.12976384e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.11211900e-03 1.69705765e-03 2.08025770e-03 1.89451161e-01]\n",
      " [2.77684173e-02 2.42444552e-01 2.53354569e-03 2.24581963e-03]\n",
      " [1.86963279e-01 5.53499735e-03 4.72049624e-03 2.01886641e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.12768279e-03 2.31258729e-02 5.41933152e-01 2.20351748e-02]\n",
      " [8.88838899e-02 9.64552568e-02 7.61175576e-01 8.33081346e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the On Policy Sarsa with maximum number of 5000 episodes we got around 42 percent accuracy which is around 55 percent increase in from the Off Policy Q - Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 after tuning hyperparameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes agent will play during training \n",
    "num_episodes = 10000\n",
    "\n",
    "# Maximum number of steps agent takes per episode\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "#Alpha - Learning Rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "#Gamma - Discounting Rate\n",
    "discount_rate = 0.99\n",
    "\n",
    "#Epsilon - Exploration Rate\n",
    "exploration_rate = 1\n",
    "\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# Exponential Decay Rate\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "\n",
    "rewards_all_episodes = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "                \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "        \n",
    "        # Add new reward        \n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Exploration rate decay   \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.048000000000000036\n",
      "2000 :  0.21900000000000017\n",
      "3000 :  0.3880000000000003\n",
      "4000 :  0.5350000000000004\n",
      "5000 :  0.6180000000000004\n",
      "6000 :  0.6420000000000005\n",
      "7000 :  0.6840000000000005\n",
      "8000 :  0.6790000000000005\n",
      "9000 :  0.6760000000000005\n",
      "10000 :  0.6790000000000005\n"
     ]
    }
   ],
   "source": [
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.49949529 0.49753489 0.50123551 0.49851384]\n",
      " [0.35179383 0.13085974 0.25594212 0.47515401]\n",
      " [0.40864376 0.43497857 0.41378101 0.46077887]\n",
      " [0.29818741 0.24431906 0.26375149 0.44408518]\n",
      " [0.51574434 0.32715581 0.28182863 0.47735402]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40934045 0.18570822 0.14658024 0.10648827]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3961589  0.47470868 0.39116167 0.5577665 ]\n",
      " [0.55517202 0.62366524 0.44584063 0.45073623]\n",
      " [0.64778268 0.41894073 0.31832293 0.33636616]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33607592 0.60998504 0.70204051 0.51451692]\n",
      " [0.69275218 0.81161666 0.78475383 0.73208297]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "****You fell through a hole!****\n"
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps_per_episode):            \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break          \n",
    "            \n",
    "    state = new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the this model with maximum number of 10000 episodes we got around 67% accuracy which around 140% of the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a baseline performance. How well did your RL Q-learning do on your problem?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By creating the Baseline perfromance the RL Q-learning gave the accuracy of 40 percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the states, the actions and the size of the Q-table?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "States:\n",
    "\n",
    "S\tAgent‚Äôs starting point - safe\t\n",
    "F\tFrozen surface         - safe\t\n",
    "H\tHole                   - game over\t\n",
    "G\tGoal                   - game over\t\n",
    "\n",
    "Actions:\n",
    "\n",
    "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole.\n",
    "\n",
    "Below grid is our Environment \n",
    "\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "The size of Q table is 16 * 4 where rows represent the States and columns represent the Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the rewards? Why did you choose them?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Rewards are 0 and 1 if agents reaches the goal he will get 1 and 0 if he falls in hole or is in safe zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did you choose alpha and gamma in the following equation? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By emperical testing and trying with different values for hyperparmeters i choosed alpha and gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try at least one additional value for alpha and gamma. How did it change the baseline performance?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using Alpha Value: 0.7 & Gamma Value: 0.8 in Baseline model accuracy came around 27 percent and \n",
    "after tuning these hyperparmeter putting the value Alpha Value: 0.1 & Gamma Value: 0.99 got accuracy 67 percent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a policy other than maxQ(s', a'). How did it change the baseline performance?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using the SARSA On Policy the baseline performace got increased from 27 percent to 42 percent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By emperical testing and trying with different values to get better perfomance selected the decay rate and epsilon.\n",
    "After changing the decay value from 0.01 in baseline to 0.001 in model 2 the accuracy was increased around 140 percent from the baseline perfomance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average number of steps taken per episode?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The average number of steps per episode on the baseline model is 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does Q-learning use value-based or policy-based iteration?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q-Learning use policy-based iteration, below is the equation\n",
    "\n",
    "ùëÑ (ùë†,ùëé )‚Üê ùëÑ (ùë†,ùëé )+ùõº (ùëü + ùõæ maxùëé‚Ä≤ùëÑ (ùë†‚Ä≤,ùëé‚Ä≤) ‚àí ùëÑ(ùë†,ùëé) )\n",
    "\n",
    "where ùëé‚Ä≤ are all actions, that were probed in state ùë†‚Ä≤according to policy ùúã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is meant by expected lifetime value in the Bellman equation?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Optimal action value function is the expected lifetime value in Bellman Equation which is \n",
    "maxùëé‚Ä≤ùëÑ (ùë†‚Ä≤,ùëé‚Ä≤),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "I have successfully implemented Baseline Model with different hyperparameters like Number of Episodes, Max steps per episode, Alpha - Learning Rate, Gamma - Discounting Rate,Epsilon - Exploration Rate, Exploration Decay Rate and found that on baseline model the accuracy was around 27 percent while after tuning the hyperparameters the accuracy increased about  percent which was around 67 percent from baseline model hyperparameters\n",
    "\n",
    "### Contributions\n",
    "\n",
    "30% of explanation and code is done by me.\n",
    "\n",
    "60-70% of resource is from web and citations are given below.\n",
    "\n",
    "10% of resource is from prof. Nik Brown notes.\n",
    "\n",
    "### Citations\n",
    "\n",
    "https://www.biostat.wisc.edu/~craven/cs760/lectures/reinforcement.pdf\n",
    "\n",
    "http://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "https://deeplizard.com/learn/video/QK_PP_2KgGE\n",
    "\n",
    "https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning\n",
    "\n",
    "\n",
    "### License\n",
    "\n",
    "#### Author Abhi Patodi\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
